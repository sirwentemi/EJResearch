---
title: "Final_Bibliometric"
author: "SirWentemi"
date: '2021-10-19'
output:
html_document: default
editor_options:
markdown:
wrap: 300
bibliography: references.bib
always_allow_html: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.retina = 2)
```

# Scoping the scholarly literature on Environmental Justice in the USA

## Background

During the summer of 2021, I worked as research fellow at the **NATIONAL ACADEMIES OF SCIENCES, ENGINEERING, AND MEDICINE**. I was tasked by **The Board on Energy and Environmental Systems** to identify topical research issues and experts related to the Environmental Justice (EJ). Environmental Justice have been central part of formulating policies in the USA. The core principle EJ driven policies is to ensure the equitable distribution of resources and minimize the disproportionate impact of specific environmental and health risk on vulnerable communities . The scholarly literature on EJ has also received major attention from a multitude of the academic disciplines due to the multifaceted nature of EJ issues in the USA.

The code and summary below provides a step-by-step process I applied to identifying salient topics and experts in the field of environmental justices. The major steps involved a bibliometric analysis to identify conceptual structure of topics and identify influential authors related to these topics. Finally I applied structural topic modeling to provide a more detailed overview topics.

### Deployment

The data for this project was sourced from the Scopus database using the keywords "environmental justice" and "environmental injustices". The keyword search was also limited to years between 1990 and 2021. To learn more about how extract publication data from Scopus please refer to this [video link](https://www.youtube.com/watch?v=vDYSIPAkKbo) .

To deploy and replicate the exercise make to install the following R packages :

1.  `bibliometrix` : used to analyze influential authors and explore the underlying concepts and topics related to EJ.
2.  `dplyr` : this package is used for data wrangling in R.
3.  `stm` : to get a deeper understanding of underlying topics of EJ literature using the abstracts.
4.  `stmCorrViz` : is a dependency of stm and used for visualizing the topics.
5.  `igraph`: used to create a network of related topics.
6.  `visNetwork`: used to create a more detail network presentation of the topics.
7.  `tidyverse` & `tidytext` : both packages were used to handle and manipulate text data.

```{r packages,message=F}
library(bibliometrix)
library(dplyr)
library(stm)
library(stmCorrViz)
library(igraph)
library(ggplot2)
library(tidytext)
library(tidyverse)
```

**Pre-processing of Scopus data set**

The `bibliometrix` package provides a seamless way of processing and extracting the meta data associated with the articles. For this project I extracted details about the authors and country of origin. In the steps below I removed duplicate articles and filtered the data to only include USA author. I also filtered the data to only include articles spanning from 2000 to 2021.

```{r, message=FALSE, warning=FALSE,results='hide'}
setwd("C:/Users/apeaning/Documents/NewEJ")
E1<-"/data/EJ_2021-2018.bib"
E2<-"/data/EJ_2017-2010.bib"
E3<-"/data/EJ_2009-1990.bib"
### Prepping data for analysis
M1 <- convert2df(E1, dbsource = "scopus", format = "bibtex")
M2 <- convert2df(E2, dbsource = "scopus", format = "bibtex")
M3 <- convert2df(E3, dbsource = "scopus", format = "bibtex")
M <- mergeDbSources(M1,M2,M3, remove.duplicated = TRUE) ### Merging and removing duplicate
M_edit <- metaTagExtraction(M, Field = "AU_CO", sep = ";")
M_edit <- metaTagExtraction(M_edit, Field = "CR_AU", sep = ";")
M_edit <- metaTagExtraction(M_edit, Field = "CR_SO", sep = ";")
M_edit <- metaTagExtraction(M_edit, Field = "AU1_CO", sep = ";")
M_edit <- metaTagExtraction(M_edit, Field = "AU_UN", sep = ";")
M_edit <- metaTagExtraction(M_edit, Field = "SR", sep = ";")
M_edit <- duplicatedMatching(M_edit, Field = "DI", tol = 0.97) ## Removing Duplicates
M_USA<-M_edit%>%filter(AU_CO=="USA"|AU1_CO=="USA")%>%filter(DE!="NA")### Filtering only USA authors and Removing duplicates
M_USA<-M_USA%>%filter(PY>1999)
```

**Overview and Summary of Articles**

The code below provides a detailed overview of articles contained data set. The high level insights include :

1.  The literature on EJ have progressively increased at rate of an annual rate of 13.9%
2.  [GRINESKI S.](https://faculty.utah.edu/u6016976-SARA_ELIZABETH_GRINESKI/hm/index.hml) and [COLLINS T.](https://faculty.utah.edu/u0201634-TIMOTHY_WILLIAM_COLLINS/research/index.hml) are most productive author in the EJ literature space with 54 and 48 articles respectively.
3.  The top 2 cited articles are [WOLCH etal 2014](https://www.sciencedirect.com/science/article/pii/S0169204614000310) and [SCHLOSBERG etal 2007](https://oxford.universitypressscholarship.com/view/10.1093/acprof:oso/9780199286294.001.0001/acprof-9780199286294) with 1488 and 1028 citation respectively.

```{r, message=FALSE, warning=FALSE}
options(width=100)
desc_usa<-biblioAnalysis(M_USA,sep=";")
summary(desc_usa)
```

**Conceptual Structure**

In the steps below , I explore the conceptual structure of topics using the authors keywords definition. To get a more resolute insight I excluded the terms "*environmental justice*" and "*sustainability".* The `conceptualStructure` function implements a natural language processing algorithm to extract and cluster terms. The function also provides options of the dimensionality reduction techniques including Multidimensional Scaling (MDS), Correspondence Analysis (CA) or Multiple Correspondence Analysis (MCA). After a number of exploratory steps I decided to use the MCA approach, since it provided the most intuitive insights about the concepts. I also set the max number of clusters to 5 based insight gleaned from the previous exploratory exercise.

The high-level insights from **conceptual structure map**, indicate that the topics in the bottom-right corner can be characterized as "climate justice" since it is closely correlated with climate change. Two closely themes emerge in the top right corner characterizing topics related to "social and justice movement" and "racial movement" of EJ. Finally, the top-left and bottom-left corner characterizes the "environmental and health concerns" and "health implications and interventions" topics of EJ research. Pivotal authors and articles related to the cluster are shown in **factorial maps** below. For example results show that article by [Spencer-Hwang etal 2016](https://muse.jhu.edu/article/644530/figure/tab04). is one of the the highest contributor to the variance of the "health implications and interventions" cluster. In addition, the paper by [O'Fallon and Dearry 2002](https://pubmed.ncbi.nlm.nih.gov/11929724/) is one of the highly ranked cited documents in the "health implications and interventions" clusters. Without going into details I was able to use this step make recommendations about the potential experts related to the themes.

```{r, message=FALSE, warning=FALSE}
CS <- conceptualStructure(M_USA, field = "DE", method = "MCA",labelsize = 15, k.max = 5,remove.terms=c("environmental justice","sustainability","environmental racism"), minDegree = 45, clust = 16)
```

**Topic modeling**

The abstract of the articles embody a rich source of high-level insights related to the studies. I decided to apply topic modeling to abstract to glean a more nuanced perspective on underlying topics related EJ literature. For this step I applied the `stm` (Structural Topic Model). According the [Roberts etal 2016](https://cran.r-project.org/web/packages/stm/index.html), structural topic modeling is a general natural language processing framework for identifying topic with document-level covariate information, which can improve inference and qualitative interpretability by affecting topical prevalence, topic content, or both.

In the steps below, I filtered abstracts and publication years associated with the articles and pre-processed the data suitable for the `stm` package. It important to note, the publications year will be operationalized as a co-variate for predicting prevalence of topics. The `textProcessor()` to stem and remove general and custom stopwords. While the `prepDocuments()` function was used to structure, index and remove lower frequency words.

```{r, message=FALSE, warning=FALSE,results='hide'}
M_lite<-M_USA%>%select(PY,AB)%>% filter(PY>2000)
#M_USA$PY
custom_stop<-c("research","analysis","taylor","francis","elsevier","find","studi*",
               "article","data","literat*","qualitati*","-*","almost","china","brazil*")
EJ_processed <- textProcessor(M_lite$AB, metadata = M_lite,customstopwords=custom_stop) 
out <- prepDocuments(EJ_processed$documents, EJ_processed$vocab, EJ_processed$meta,lower.thresh=0.2)
docs <- out$documents
vocab <- out$vocab
meta <- out$meta
```

The code below executes the topic modeling using the publication years are co-variate to predict the prevalence of the topic. The breakdown of the top 5 words associate with the 10 topics is shown the graphs below. Clearly the results from the topic model provides a more naunced perspective on

```{r, message=FALSE, warning=FALSE,results='hide'}
model_fit <- stm(documents = out$documents, vocab = out$vocab, K = 10, max.em.its = 75, data = out$meta,prevalence = ~as.numeric(PY), init.type = "Spectral")
td_beta <- tidy(model_fit)

td_beta %>%
    group_by(topic) %>%
    top_n(5, beta) %>%
    ungroup() %>%
    mutate(topic = paste0("Topic ", topic),
           term = reorder_within(term, beta, topic)) %>%
    ggplot(aes(term, beta, fill = as.factor(topic))) +
    geom_col(alpha = 0.8, show.legend = FALSE) +
    facet_wrap(~ topic, scales = "free_y",ncol=5) +
    coord_flip() +
    scale_x_reordered() +
    labs(x = NULL, y = expression(beta),
         title = "Highest word probabilities for each topic",
         subtitle = "Different words are associated with EJ topics")
```

```{r}
library(igraph); library(visNetwork)
k=10
mod.out.corr <- topicCorr(model_fit,method="simple",cutoff=0.0)
topicNames <- labelTopics(model_fit, n =10)
topic <- data.frame(
  TopicNumber = 1:k,
  TopicProportions = colMeans(model_fit$theta))


mod.out.corr <- topicCorr(model_fit, cutoff = .001)
# output links and simplify
links2 <- as.matrix(mod.out.corr$posadj)
net2 <- graph_from_adjacency_matrix(links2, mode = "undirected")
net2 <- igraph::simplify(net2) 
# create the links and nodes
links <- igraph::as_data_frame(net2, what="edges")
nodes <- igraph::as_data_frame(net2, what="vertices")
# set parameters for the network
nodes$shape <- "dot"  
nodes$title <- paste0("Topic ", topic$TopicNumber)
nodes$label <- apply(topicNames$prob, 1, function(x) paste0(x, collapse = " \n ")) # Node label
nodes$size <- (topic$TopicProportions / max(topic$TopicProportions)) * 20
nodes$font <- "18px"
nodes$id <- as.numeric(1:k)
visNetwork(nodes, links, width="100%",  height="800px", main="EJ Topic Modeling") %>% 
  visEdges(arrows = "to") %>% visOptions(collapse = list(enabled = TRUE, clusterOptions = list(shape = "square")))%>% 
  visHierarchicalLayout(direction = "LR") 
```

```{r}
extra_fit <- estimateEffect(1:10 ~PY, model_fit, meta=out$meta,uncertainty="None")
plot(extra_fit, "PY", method="continuous", topics=extra_fit$topics[3], printlegend=FALSE, xaxt="s",xlab="Period [2000-2021]")
```
